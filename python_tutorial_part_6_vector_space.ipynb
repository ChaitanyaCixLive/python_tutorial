{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A Vector Space Model, with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is code to build a vector space model, with SVMs on Andrew Mass' \n",
    "# distribution of movie review sentiment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female\n",
      "Visual Arts\n"
     ]
    }
   ],
   "source": [
    "# While Python tuples is indexed numerically (like a list), a named tuple assigns names to fields and \n",
    "# is also indexed numerically. This makes it possible to access the fields in a named tuple using these names\n",
    "# as if they were attributes of an object (via dotting into the namedtuple)\n",
    "# See also here: https://docs.python.org/2/library/collections.html\n",
    "from collections import namedtuple\n",
    "Student = namedtuple(\"Student\", [\"name\", \"age\", \"gender\", \"course\"])\n",
    "#--------------------------------------------------------------------\n",
    "# Note: You can also provide field names as a space-delimited string, rather than a list.\n",
    "#Student = namedtuple(\"Student\", \"name age gender course\")\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "angela=Student(name=\"Angela\", age=45, gender=\"female\", course=\"Python\")\n",
    "soha=Student(name=\"Soha\", age=25, gender=\"female\", course=\"Visual Arts\")\n",
    "print(angela.gender)\n",
    "print(soha.course)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soha\n",
      "25\n",
      "female\n",
      "Visual Arts\n"
     ]
    }
   ],
   "source": [
    "# A namedtuple is also iterable like a tuple\n",
    "for i in soha:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Visual Arts'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can access a namedtuple the same way you access a tuple or a list:\n",
    "soha[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student(name='Angela', age=45, gender='female', course='Python')\n",
      "Student(name='Soha', age=25, gender='female', course='Visual Arts')\n"
     ]
    }
   ],
   "source": [
    "# We can now create a list where we append the two namedtuples above.\n",
    "# i.e., a list of namedtuples\n",
    "all_students=[]\n",
    "all_students.append(angela)\n",
    "all_students.append(soha)\n",
    "for s in all_students:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Angela is 45 years old.\n",
      "- Soha is 25 years old.\n"
     ]
    }
   ],
   "source": [
    "for s in all_students:\n",
    "    print(\"- {} is {} years old.\").format(s.name, s.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We should usually get tags automatically based on input data file.\n",
    "# In the input data file we have, we know that the first 12500 data points are positive/1.0 and the next 12500 are\n",
    "# negative/0.0 then the next 12500 is poitive and the fourth chunk is negative.\n",
    "# So basically the train_data has 25K (with the first half positive and the second half negative)\n",
    "# and test_data with the same setup for class label. \n",
    "# The rest of the data in the file is unknown/neutral/-1 and we don't use that part.\n",
    "\n",
    "def map_tags(post_index):\n",
    "    # if post is positive, tag=1, if it is negative tag=0, if it is neutral, tag=-1\n",
    "    tag=-1\n",
    "    if post_index < 12500:\n",
    "        tag=1\n",
    "    elif post_index < 25000:\n",
    "        tag=0\n",
    "    elif post_index < 37500:\n",
    "        tag=1\n",
    "    elif post_index < 50000:\n",
    "        tag=0\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "**************************************************\n",
      "DataDoc(tag=1, words=['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'teachers', '\"', '.', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'teachers', '\"', '.', 'the', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'i', 'immediately', 'recalled', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'at', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'high', '.', 'a', 'classic', 'line', ':', 'inspector', ':', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'student', ':', 'welcome', 'to', 'bromwell', 'high', '.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', '.', 'what', 'a', 'pity', 'that', 'it', \"isn't\", '!'])\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def get_all_data():\n",
    "    \"\"\"\n",
    "    Returns a list of namedtuples from the IMDB file.\n",
    "    Each namedtuple has two named fields:\n",
    "        tag= class label (0 for \"negative\" and 1 for \"positive\")\n",
    "        word_list the list of words in the review\n",
    "    \"\"\"\n",
    "    # a list to house all the data\n",
    "    all_data = []  \n",
    "    \n",
    "    DataDoc= namedtuple('DataDoc', 'tag words')\n",
    "    with open('/Users/mam/CORE/RESEARCH/DEEPLEARNING/Doc2Vec/data/aclImdb/alldata-id.txt') as alldata:\n",
    "        for line_no, line in enumerate(alldata):\n",
    "            post_index=int(line.split()[0].split(\"*\")[-1])\n",
    "            label=map_tags(post_index)\n",
    "            word_list=line.lower().split()[1:]\n",
    "            all_data.append(DataDoc(label, word_list))\n",
    "    return all_data\n",
    "\n",
    "# Call the function to get the data\n",
    "all_data= get_all_data()\n",
    "# The data are 100K reviews as explained earlier\n",
    "# Since the last 50K are unknown, let's throw them away\n",
    "all_data=all_data[:50000]\n",
    "print(len(all_data))\n",
    "print(\"*\"*50)\n",
    "# print the first namedtuple\n",
    "print(all_data[0])\n",
    "print(\"*\"*50)\n",
    "# print the last namedtuple\n",
    "#print(all_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# The data set is big, and we want to only work with a very small sample of it.\n",
    "# Let's randomize the reviews and then take only 500 of them and call them train_data.\n",
    "# We will then do cross-validation on these later.\n",
    "from random import shuffle\n",
    "shuffle(all_data)\n",
    "#-------------------------\n",
    "train_data = all_data[:500]\n",
    "#------------------------\n",
    "print len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13848\n",
      "13828\n"
     ]
    }
   ],
   "source": [
    "# Let's get a dictionary of all the words in training data\n",
    "# These will be our bag-of-words features\n",
    "# We won't need this function, since we will use gensim's built-in method \"Dictionary\" from the corpus module\n",
    "# --> corpora.Dictionary, but we provide this so that you are clear on one way of how to do this.\n",
    "from collections import defaultdict\n",
    "def get_space(train_data):\n",
    "    \"\"\"\n",
    "    input is a list of namedtuples\n",
    "    get a dict of word space\n",
    "    key=word\n",
    "    value=len of the dict at that point \n",
    "    (that will be the index of the word and it is unique since the dict grows as we loop)\n",
    "    \"\"\"\n",
    "    word_space=defaultdict(int)\n",
    "    for doc in train_data:\n",
    "        for w in doc.words:\n",
    "            # indexes of words won't be in sequential order as they occur in data (can you tell why?), \n",
    "            # but that doesn't matter.\n",
    "            word_space[w]=len(word_space)\n",
    "    return word_space\n",
    "\n",
    "word_space=get_space(train_data)\n",
    "print len(word_space)\n",
    "print word_space[\"love\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sparse_vec(data_point, space):\n",
    "    # create empty vector\n",
    "    sparse_vec = np.zeros((len(space)))\n",
    "    for w in set(data_point.words):\n",
    "        # use exception handling such that this function can also be used to vectorize \n",
    "        # data with words not in train (i.e., test and dev data)\n",
    "        try:\n",
    "            sparse_vec[space[w]]=1\n",
    "        except:\n",
    "            continue\n",
    "    return sparse_vec\n",
    "\n",
    " \n",
    "\n",
    "train_vecs= [get_sparse_vec(data_point, word_space) for data_point in train_data]\n",
    "# Get class labels\n",
    "train_tags=[train_data[i].tag for i in range(len(train_data))]\n",
    "# Let's look at the last training data point\n",
    "print train_tags[-1], train_vecs[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 13848)\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn likes to take data as numpy arrays. So, let's change our data accordingly:\n",
    "train_vecs=np.array(train_vecs)\n",
    "train_tags=np.array(train_tags)\n",
    "print train_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "\n",
      "Done fitting classifier on training data...\n",
      "\n",
      "================================================== \n",
      "\n",
      "Results with 5-fold cross validation:\n",
      "\n",
      "================================================== \n",
      "\n",
      "********************\n",
      "\t accuracy_score\t0.644\n",
      "********************\n",
      "precision_score\t0.657692307692\n",
      "recall_score\t0.657692307692\n",
      "\n",
      "classification_report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.63      0.63       240\n",
      "          1       0.66      0.66      0.66       260\n",
      "\n",
      "avg / total       0.64      0.64      0.64       500\n",
      "\n",
      "\n",
      "confusion_matrix:\n",
      "\n",
      "[[151  89]\n",
      " [ 89 171]]\n"
     ]
    }
   ],
   "source": [
    "# Classification with scikit-learn\n",
    "# Now we have: train_tags, train_vecs, test_tags, test_vecs\n",
    "# Let's use sklearn to train an svm classifier:\n",
    "#-------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import codecs\n",
    "import time\n",
    "import sys\n",
    "import os, re, glob\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from random import shuffle, randint\n",
    "import numpy as np\n",
    "from numpy import array, arange, zeros, hstack, argsort\n",
    "import unicodedata\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "import gensim\n",
    "n_jobs = 2\n",
    "\n",
    "#train_vecs=array(train_vecs)\n",
    "train_vecs=np.array(train_vecs)\n",
    "train_tags=np.array(train_tags)\n",
    "\n",
    "print type(train_tags)\n",
    "print type(train_vecs)\n",
    "clf = OneVsRestClassifier(SVC(C=1, kernel = 'linear', gamma=1, verbose= False, probability=False))\n",
    "clf.fit(train_vecs, train_tags)\n",
    "print \"\\nDone fitting classifier on training data...\\n\"\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "print \"=\"*50, \"\\n\"\n",
    "print \"Results with 5-fold cross validation:\\n\"\n",
    "print \"=\"*50, \"\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "predicted = cross_validation.cross_val_predict(clf, train_vecs, train_tags, cv=5)\n",
    "print \"*\"*20\n",
    "print \"\\t accuracy_score\\t\", metrics.accuracy_score(train_tags, predicted)\n",
    "print \"*\"*20\n",
    "print \"precision_score\\t\", metrics.precision_score(train_tags, predicted)\n",
    "print \"recall_score\\t\", metrics.recall_score(train_tags, predicted)\n",
    "print \"\\nclassification_report:\\n\\n\", metrics.classification_report(train_tags, predicted)\n",
    "print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(train_tags, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52\n"
     ]
    }
   ],
   "source": [
    "# Usually, we calculate a basline as the majority class in training data.\n",
    "# Here, to simplify, we just get the majority class in all the data (see support, which is the number of data points in each\n",
    "# class, in the classification report above)\n",
    "majority_class=260/500.0\n",
    "print(majority_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
