{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also see:  \n",
    "## http://www.nltk.org/book/ch03.html, https://docs.python.org/2/howto/urllib2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a book from Project Gutenberg with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'response' is <type 'instance'>:\n",
      "Type of 'raw' is <type 'unicode'>:\n"
     ]
    }
   ],
   "source": [
    "from urllib2 import Request, urlopen\n",
    "\n",
    "url=\"http://www.gutenberg.org/files/54255/54255-0.txt\"\n",
    "response = urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "#--------------------------------------------------\n",
    "# Check types...\n",
    "print(\"Type of \\'response\\' is %s:\")% type(response)\n",
    "print(\"Type of \\'raw\\' is %s:\")% type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Narrative of Travels in Europe, Asia, and\r\n",
      "Africa, in the Seventeenth Century, Volum, by Evliya Çelebi and Joseph Hammer-Purgstall\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "print(raw[:165])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', ',', 'guys', ',', 'how', 'is', 'life', '?', '?', '?', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "t=\"hey, guys, how is life???!\"\n",
    "tt =word_tokenize(t)\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hey', 'NN'), (',', ','), ('guys', 'NNS'), (',', ','), ('how', 'WRB'), ('is', 'VBZ'), ('life', 'NN'), ('?', '.'), ('?', '.'), ('?', '.'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "ttt = pos_tag(tt)\n",
    "print(ttt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and pos-tag the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144822\n",
      "144822\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "#------------------------------\n",
    "tokens = word_tokenize(raw)\n",
    "print(len(tokens))\n",
    "tagged=pos_tag(tokens)\n",
    "print(len(tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\ufeffThe', u'Project', u'Gutenberg', u'EBook', u'of', u'Narrative', u'of', u'Travels', u'in', u'Europe', u',', u'Asia', u',', u'and', u'Africa', u',', u'in', u'the', u'Seventeenth', u'Century', u',', u'Volum', u',', u'by', u'Evliya', u'\\xc7elebi', u'and', u'Joseph', u'Hammer-Purgstall', u'This', u'eBook', u'is', u'for', u'the', u'use', u'of', u'anyone', u'anywhere', u'in', u'the', u'United', u'States', u'and', u'most', u'other', u'parts', u'of', u'the', u'world', u'at']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:50]) # list of unicode items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'\\ufeffThe', 'NN'), (u'Project', 'NNP'), (u'Gutenberg', 'NNP'), (u'EBook', 'NNP'), (u'of', 'IN'), (u'Narrative', 'NNP'), (u'of', 'IN'), (u'Travels', 'NNP'), (u'in', 'IN'), (u'Europe', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged[:10]) # list of tuples (word,pos_tag pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'hi']\n"
     ]
    }
   ],
   "source": [
    "wds=[\"hello\", \"hi\", \"life\"]\n",
    "h_wds= [w for w in wds if w.startswith(\"h\")]\n",
    "\n",
    "\n",
    "new_words=[]\n",
    "for w in wds:\n",
    "    if w.startswith(\"h\"):\n",
    "        new_words.append(w)\n",
    "print(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plays']\n"
     ]
    }
   ],
   "source": [
    "pairs=[  (\"Alex\", \"NN\"), (\"plays\", \"VBZ\")    ]\n",
    "verbs=[  x[0] for x in pairs  if x[1]==\"VBZ\"]\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: The pos tagger of course makes mistakes, but it performs reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List comprehension on \"tagged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project\n",
      "Gutenberg\n",
      "EBook\n",
      "Narrative\n",
      "Travels\n",
      "Europe\n",
      "Asia\n",
      "Africa\n",
      "Seventeenth\n",
      "Century\n",
      "Volum\n",
      "Evliya\n",
      "Çelebi\n",
      "Joseph\n",
      "Hammer-Purgstall\n",
      "United\n",
      "Project\n",
      "Gutenberg\n",
      "License\n",
      "United\n",
      "Europe\n",
      "Asia\n",
      "Africa\n",
      "Seventeenth\n",
      "Century\n",
      "II\n",
      "Evliya\n",
      "Çelebi\n",
      "Evliya\n",
      "Çelebi\n",
      "Joseph\n",
      "Hammer-Purgstall\n",
      "Release\n",
      "Date\n",
      "February\n",
      "[\n",
      "EBook\n",
      "Character\n",
      "***\n",
      "START\n",
      "THIS\n",
      "PROJECT\n",
      "GUTENBERG\n",
      "EBOOK\n",
      "NARRATIVE\n",
      "OF\n",
      "TRAVELS\n",
      "***\n",
      "Produced\n",
      "Turgut\n"
     ]
    }
   ],
   "source": [
    "# Named enitities:\n",
    "ne=[pair[0] for pair in tagged if pair[-1]==\"NNP\"]\n",
    "for e in ne[:50]:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breadth\n",
      "remarkable\n",
      "ruby-coloured\n",
      "particular\n",
      "tombs\n",
      "gun-shot’s\n",
      "yellow\n",
      "rapid\n",
      "mild\n",
      "mile\n",
      "sleep\n",
      "legal\n",
      "forty-six\n",
      "Elephant\n",
      "dish\n",
      "follow\n",
      "abundant\n",
      "religious\n",
      "washing-tubs\n",
      "dreadful\n",
      "seventy-seven\n",
      "pardon\n",
      "hunting\n",
      "swam\n",
      "outdated\n",
      "becas\n",
      "mosque\n",
      "young\n",
      "“Mevlúd-námeh\n",
      "underwent\n",
      "answered\n",
      "tail\n",
      "foster\n",
      "obstinate\n",
      "stable\n",
      "suite\n",
      "Precious\n",
      "farsang’s\n",
      "worth\n",
      "orderly\n",
      "virtuous\n",
      "Sheikh-ul-islám\n",
      "amorous\n",
      "exempt\n",
      "www.gutenberg.org\n",
      "perishable\n",
      "navigable\n",
      "limpid\n",
      "fat\n",
      "father’s\n"
     ]
    }
   ],
   "source": [
    "# Adjectives\n",
    "adjs= set([pair[0] for pair in tagged if pair[-1]==\"JJ\"]) # we pass the list to set to uniqify\n",
    "adjs= list(adjs) #Cast to list again so that we access only few in print\n",
    "# Note: 'set' object has no attribute '__getitem__' and so we cannot do adjs[:15] on a set\n",
    "for a in adjs[:50]:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15238\n"
     ]
    }
   ],
   "source": [
    "# How many ne?; note these are not uniqified\n",
    "print(len(ne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1263\n"
     ]
    }
   ],
   "source": [
    "# How many uniqe adjs?\n",
    "print(len(adjs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg-tm; three hundred; hundred houses; Black Sea;\n",
      "thousand men; two hundred; one hundred; great number; fifty aspers;\n",
      "next day; Project Gutenberg; Uzún Hassan; three days; thousand houses;\n",
      "five hours; Sultán Murad; Ahmed Páshá; Kizil Irmák; five hundred;\n",
      "Mustafa Páshá\n"
     ]
    }
   ],
   "source": [
    "from nltk import Text\n",
    "text=Text(tokens)\n",
    "#print(type(text))\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing webpages/html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" id=\"responsive-news\">\n",
      "<head  prefix=\"og: http://ogp.me/ns#\">\n",
      "    <meta charset=\"utf-8\">\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\">\n",
      "    <title>Sh\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "url=\"http://www.bbc.com/news/technology-38892383\"\n",
    "response = urlopen(url)\n",
    "html = response.read().decode('utf8')\n",
    "print(html[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shopping\n",
      "robots\n",
      "on\n",
      "the\n",
      "march\n",
      "in\n",
      "Ocado\n",
      "-\n",
      "BBC\n",
      "News\n"
     ]
    }
   ],
   "source": [
    "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "tok=tokens[:10]\n",
    "for t in tok:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2167789138\tمعَ فجر العام الجديد : رجوتُ إلهيَ أن يجعلني ويجعلكمِ من أسعدِ خلقهِ ، و يرزقني ويرزقكم أضعاافَ أمنيآتِكم حتَى ترضون ...صباحكم رضى||$||\"@jumana_sj2: ولا اقول عاادي كمان يدخلو اغاني كوريه خنضحك #عشاق_كوريا_يطالبون_قناة_mbc_بفتح_قناه_mbc_korea_مترجمه_بالعربيه_للمعجبين_العرب\"||$||\"@s_h_osho: نبي قناه كوريه ليش فيه قناه هنديه ومافيه كوريه؟ #عشاق_كوريا_يطالبون_قناة_mbc_بفتح_قناه_mbc_korea_مترجمه_بالعربيه_للمعجبين_العرب\"||$||\"@LINA_ALADEEB: بعيداً عن خيالات الحب احياناً السعاده تكون عباره ع\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "ara_text=codecs.open(\"sample_concat.tsv\", \"r\", \"utf-8\").readlines()[0]\n",
    "print(ara_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2167789138\tمع فجر العام الجديد : رجوت إلهي أن يجعلني ويجعلكم من أسعد خلقه ، و يرزقني ويرزقكم أضعااف أمنيآتكم حتى ترضون ...صباحكم رضى||$||\"@jumana_sj2: ولا اقول عاادي كمان يدخلو اغاني كوريه خنضحك #عشاق_كوريا_يطالبون_قناة_mbc_بفتح_قناه_mbc_korea_مترجمه_بالعربيه_للمعجبين_العرب\"||$||\"@s_h_osho: نبي قناه كوريه ليش فيه قناه هنديه ومافيه كوريه؟ #عشاق_كوريا_يطالبون_قناة_mbc_بفتح_قناه_mbc_korea_مترجمه_بالعربيه_للمعجبين_العرب\"||$||\"@LINA_ALADEEB: بعيدا عن خيالات الحب احيانا السعاده تكون عباره عن - برنامج \n"
     ]
    }
   ],
   "source": [
    "def remove_unicode_diac(text):\n",
    "    \"\"\"Takes Arabic in utf-8 and returns same text without diac\"\"\"\n",
    "    # Replace diacritics with nothing \n",
    "    text = text.replace(u\"\\u064B\", \"\")# fatHatayn\n",
    "    text = text.replace(u\"\\u064C\", \"\") # Dammatayn\n",
    "    text = text.replace(u\"\\u064D\", \"\")# kasratayn\n",
    "    text = text.replace(u\"\\u064E\", \"\")# fatHa\n",
    "    text = text.replace(u\"\\u064F\", \"\") # Damma\n",
    "    text = text.replace(u\"\\u0650\", \"\")# kasra\n",
    "    text = text.replace(u\"\\u0651\", \"\")# shaddah\n",
    "    text = text.replace(u\"\\u0652\", \"\")# sukuun\n",
    "    text = text.replace(u\"\\u0670\", \"`\") # dagger 'alif\n",
    "    return text\n",
    "\n",
    "ara_text_no_diac =remove_unicode_diac(ara_text)\n",
    "print(ara_text_no_diac[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "print(type(ara_text_no_diac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions preview!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there, take a look: <URL> #love_robots!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# This will replace the URL \"http://www.bbc.com/news/technology-38892383\" with a string token \"<URL>\"\n",
    "tweet=\"Hey there, take a look: http://www.bbc.com/news #love_robots!\"\n",
    "tweet = re.sub(r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', '<URL>',tweet)\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['take']\n"
     ]
    }
   ],
   "source": [
    "e_ending=[w for w in tweet.split() if re.search('e$', w)]\n",
    "print(e_ending) # Note that \"there,\" ends in \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'take']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punc = [char for char in string.punctuation]\n",
    "def clean_punc(punc, text):\n",
    "    for i in punc:\n",
    "        text=text.replace(i, \"\")\n",
    "    return text\n",
    "\n",
    "tweet=clean_punc(punc, tweet)\n",
    "e_ending=[w for w in tweet.split() if re.search('e$', w)]\n",
    "print(e_ending) # Note that \"there,\" ends in \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "print(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'hey people')\n",
      "(1, 'how are you?')\n",
      "(2, 'life is good!')\n"
     ]
    }
   ],
   "source": [
    "alldata=[\"hey people\", \"how are you?\", \"life is good!\"]\n",
    "for line_no, line in enumerate(alldata):\n",
    "    print(line_no, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'teachers', '\"', '.', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'teachers', '\"', '.', 'the', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'i', 'immediately', 'recalled', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'at', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'high', '.', 'a', 'classic', 'line', ':', 'inspector', ':', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'student', ':', 'welcome', 'to', 'bromwell', 'high', '.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', '.', 'what', 'a', 'pity', 'that', 'it', \"isn't\", '!']\n"
     ]
    }
   ],
   "source": [
    "line=\"\"\"_*0 bromwell high is a cartoon comedy .  it ran at the same time as some other programs about school life ,  such as  \" teachers \"  .  my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is  \" teachers \"  .  the scramble to survive financially ,  the insightful students who can see right through their pathetic teachers' pomp ,  the pettiness of the whole situation ,  all remind me of the schools i knew and their students .  when i saw the episode in which a student repeatedly tried to burn down the school ,  i immediately recalled  .  .  .  .  .  .  .  .  .  at  .  .  .  .  .  .  .  .  .  .  high .  a classic line :  inspector :  i'm here to sack one of your teachers .  student :  welcome to bromwell high .  i expect that many adults of my age think that bromwell high is far fetched .  what a pity that it isn't ! \"\"\"\n",
    "line.split()[0]\n",
    "words=line.split()[1:]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "space=defaultdict(int)\n",
    "for w in words:\n",
    "    space[w]=len(space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'!': 96,\n",
       "             '\"': 41,\n",
       "             ',': 74,\n",
       "             '.': 93,\n",
       "             '35': 25,\n",
       "             ':': 85,\n",
       "             'a': 94,\n",
       "             'about': 17,\n",
       "             'adults': 88,\n",
       "             'age': 89,\n",
       "             'all': 59,\n",
       "             'and': 64,\n",
       "             'as': 22,\n",
       "             'at': 76,\n",
       "             'believe': 33,\n",
       "             'bromwell': 91,\n",
       "             'burn': 72,\n",
       "             'can': 47,\n",
       "             'cartoon': 4,\n",
       "             'classic': 76,\n",
       "             'closer': 38,\n",
       "             'comedy': 5,\n",
       "             'down': 73,\n",
       "             'episode': 67,\n",
       "             'expect': 86,\n",
       "             'far': 91,\n",
       "             'fetched': 92,\n",
       "             'financially': 43,\n",
       "             'here': 81,\n",
       "             'high': 91,\n",
       "             \"high's\": 35,\n",
       "             'i': 86,\n",
       "             \"i'm\": 80,\n",
       "             'immediately': 74,\n",
       "             'in': 68,\n",
       "             'insightful': 44,\n",
       "             'inspector': 79,\n",
       "             'is': 91,\n",
       "             \"isn't\": 95,\n",
       "             'it': 95,\n",
       "             'knew': 63,\n",
       "             'lead': 30,\n",
       "             'life': 19,\n",
       "             'line': 77,\n",
       "             'many': 87,\n",
       "             'me': 61,\n",
       "             'much': 37,\n",
       "             'my': 89,\n",
       "             'of': 89,\n",
       "             'one': 83,\n",
       "             'other': 15,\n",
       "             'pathetic': 52,\n",
       "             'pettiness': 55,\n",
       "             'pity': 94,\n",
       "             'pomp': 54,\n",
       "             'profession': 29,\n",
       "             'programs': 16,\n",
       "             'ran': 8,\n",
       "             'reality': 39,\n",
       "             'recalled': 75,\n",
       "             'remind': 60,\n",
       "             'repeatedly': 70,\n",
       "             'right': 49,\n",
       "             'sack': 82,\n",
       "             'same': 11,\n",
       "             'satire': 36,\n",
       "             'saw': 66,\n",
       "             'school': 74,\n",
       "             'schools': 61,\n",
       "             'scramble': 41,\n",
       "             'see': 48,\n",
       "             'situation': 58,\n",
       "             'some': 14,\n",
       "             'student': 85,\n",
       "             'students': 65,\n",
       "             'such': 21,\n",
       "             'survive': 42,\n",
       "             'teachers': 85,\n",
       "             \"teachers'\": 53,\n",
       "             'teaching': 28,\n",
       "             'than': 40,\n",
       "             'that': 95,\n",
       "             'the': 74,\n",
       "             'their': 65,\n",
       "             'think': 90,\n",
       "             'through': 50,\n",
       "             'time': 12,\n",
       "             'to': 86,\n",
       "             'tried': 71,\n",
       "             'welcome': 85,\n",
       "             'what': 93,\n",
       "             'when': 65,\n",
       "             'which': 68,\n",
       "             'who': 46,\n",
       "             'whole': 57,\n",
       "             'years': 26,\n",
       "             'your': 84})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vec = np.zeros(len(space))\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  0.  0.  1.  1.  0.  1.  1.  1.  1.\n",
      "  0.  1.  0.  1.  1.  0.  0.  1.  1.  0.  1.  1.  1.  0.  0.  1.  0.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  1.\n",
      "  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    vec[space[w]]=1\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "cd\n"
     ]
    }
   ],
   "source": [
    "x=[\"a\", \"ab\", \"abc\", \"cd\", \"xxx\"]\n",
    "for i in x:\n",
    "    if \"c\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'cd']\n"
     ]
    }
   ],
   "source": [
    "c_list=[i for i in x if \"c\" in i]\n",
    "print(c_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
